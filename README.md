# üß† Flask + LangChain + Local LLM Demo

D·ª± √°n n√†y minh h·ªça c√°ch x√¢y d·ª±ng API s·ª≠ d·ª•ng **Flask** l√†m backend, **LangChain** ƒë·ªÉ qu·∫£n l√Ω m√¥ h√¨nh ng√¥n ng·ªØ, v√† **llama-cpp-python** ƒë·ªÉ ch·∫°y m√¥ h√¨nh LLM local (offline, kh√¥ng c·∫ßn OpenAI API).

---

## üöÄ 1. Y√™u c·∫ßu h·ªá th·ªëng

- Python 3.13 ho·∫∑c cao h∆°n  
- pip (ƒë√£ c√†i s·∫µn trong Python)  
- M√°y c√≥ √≠t nh·∫•t **8GB RAM** n·∫øu mu·ªën ch·∫°y LLM local  
- (T√πy ch·ªçn) GPU h·ªó tr·ª£ tƒÉng t·ªëc inference (n·∫øu d√πng llama.cpp b·∫£n c√≥ CUDA)

---

## üì¶ 2. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng

Clone ho·∫∑c t·∫£i project v·ªÅ:
```bash
git clone https://github.com/<your-repo>/flask-langchain-llm.git
cd flask-langchain-llm
Windows:: .venv\Scripts\activate
macOS / Linux:: source venv/bin/activate
pip install -r requirements.txt
python app.py
